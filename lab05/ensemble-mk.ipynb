{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ec9f3f",
   "metadata": {},
   "source": [
    "#### **Ensemble Lab**\n",
    "Mahitha\n",
    "\n",
    "**Date: 4/19/2025**\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "\n",
    "In this lab, we explore the application of various ensemble machine learning models to classify wine quality using the winequality-red dataset. The dataset contains physicochemical properties of red wine samples, such as acidity, sugar content, and alcohol percentage, along with a quality score rated by wine tasters. To simplify the analysis, the quality scores are categorized into three classes: low, medium, and high.\n",
    "\n",
    "The primary objective of this lab is to compare the performance of different ensemble models, including Random Forest, Gradient Boosting, AdaBoost, and Voting Classifiers, among others. We evaluate these models based on metrics such as accuracy, F1 score, and their ability to generalize to unseen data. By analyzing the results, we aim to identify the most effective model for predicting wine quality while balancing performance and generalization.\n",
    "\n",
    "This lab also emphasizes the importance of feature selection, data preprocessing, and hyperparameter tuning in building robust machine learning models. Through this hands-on approach, we gain insights into the strengths and limitations of ensemble methods in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d6acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# ------------------------------------------------\n",
    "# Imports once at the top, organized\n",
    "# ------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d40f7",
   "metadata": {},
   "source": [
    "#### **Section 1. Load and Inspect the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722c72ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (download from UCI and save in the same folder)\n",
    "df = pd.read_csv(r'C:\\Users\\Mahi2\\projects\\applied-ml-mk\\applied-ml-mk\\lab05\\wine\\winequality-red.csv', sep=';')\n",
    "# Display structure and first few rows\n",
    "df.info()\n",
    "df.head(5)\n",
    "\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae634d60",
   "metadata": {},
   "source": [
    "#### **Section 2. Prepare the Data**\n",
    "\n",
    "Includes cleaning, feature engineering, encoding, splitting, helper functions\n",
    "\n",
    "#### **Define helper function that:**\n",
    "\n",
    "**Takes one input, the quality (which we will temporarily name q while in the function)**\n",
    "\n",
    "**And returns a string of the quality label (low, medium, high)**\n",
    "\n",
    "**This function will be used to create the quality_label column**\n",
    "\n",
    "**def quality_to_label(q): if q <= 4: return \"low\" elif q <= 6: return \"medium\" else: return \"high\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07a60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert quality score to label\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "\n",
    "# Function to convert quality score to a numeric value\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Apply the function to create a numeric column\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b0efb",
   "metadata": {},
   "source": [
    "#### **Section 3. Feature Selection and Justification**\n",
    "\n",
    "**Define input features (X) and target (y)**\n",
    "\n",
    "**Features: all columns except 'quality' and 'quality_label' and 'quality_numberic' - drop these from the input array**\n",
    "\n",
    "**Target: quality_label (the new column we just created)**\n",
    "\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"]) # Features y = df[\"quality_numeric\"] # Target\n",
    "\n",
    "Explain / introduce your choices:\n",
    "\n",
    "We want to train only on physicochemical properties of the wine (like acidity, pH, alcohol content, etc. We’re treating this as a multi-class classification problem where we want to train a model to predict one of three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1a933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 4. Split the Data into Train and Test\n",
    "# Train/test split (stratify to preserve class balance)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"]) ##Features\n",
    "y = df[\"quality_numeric\"] # Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80cdad",
   "metadata": {},
   "source": [
    "#### **Section 5. Evaluate Model Performance (Choose 2)**\n",
    "\n",
    "Below is a list of 9 model variations. Choose two to focus on for your comparison.\n",
    "\n",
    "Option Model Name Notes 1 Random Forest (100) A strong baseline model using 100 decision trees.\n",
    "\n",
    "2 Random Forest (200, max_depth=10) Adds more trees, but limits tree depth to reduce overfitting.\n",
    "\n",
    "3 AdaBoost (100) Boosting method that focuses on correcting previous errors.\n",
    "\n",
    "4 AdaBoost (200, lr=0.5) More iterations and slower learning for better generalization.\n",
    "\n",
    "5 Gradient Boosting (100) Boosting approach using gradient descent.\n",
    "\n",
    "6 Voting (DT + SVM + NN) Combines diverse models by averaging their predictions.\n",
    "\n",
    "7 Voting (RF + LR + KNN) Another mix of different model types.\n",
    "\n",
    "8 Bagging (DT, 100) Builds many trees in parallel on different samples.\n",
    "\n",
    "9 MLP Classifier A basic neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c01c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 256   8]\n",
      " [  0  15  28]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8875\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8661\n",
      "\n",
      "Gradient Boosting (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  3 247  14]\n",
      " [  0  16  27]]\n",
      "Train Accuracy: 0.9601, Test Accuracy: 0.8562\n",
      "Train F1 Score: 0.9584, Test F1 Score: 0.8411\n"
     ]
    }
   ],
   "source": [
    "# Helper function to train and evaluate models\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Train F1\": train_f1,\n",
    "            \"Test F1\": test_f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "######################################\n",
    "#Here's how to create the different types of \n",
    "# ensemble models listed above \n",
    "# (you don't need to do all of them yourself. \n",
    "# Choose 2 - we have a whole team working on this.)\n",
    "######################################\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Random Forest\n",
    "evaluate_model(\n",
    "    \"Random Forest (100)\",\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "# 2. Random Forest (200, max depth=10) \n",
    "# evaluate_model(\n",
    "#     \"Random Forest (200, max_depth=10)\",\n",
    "#     RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     results,\n",
    "# )\n",
    "\n",
    "# 3. AdaBoost \n",
    "# evaluate_model(\n",
    "#     \"AdaBoost (100)\",\n",
    "#     AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     results,\n",
    "# )\n",
    "\n",
    "# 4. AdaBoost (200, lr=0.5) \n",
    "# evaluate_model(\n",
    "#     \"AdaBoost (200, lr=0.5)\",\n",
    "#     AdaBoostClassifier(n_estimators=200, learning_rate=0.5, random_state=42),\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     results,\n",
    "# )\n",
    "\n",
    "# 5. Gradient Boosting\n",
    "evaluate_model(\n",
    "    \"Gradient Boosting (100)\",\n",
    "    GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    ),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "# 6. Voting Classifier (DT, SVM, NN) \n",
    "# voting1 = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         (\"DT\", DecisionTreeClassifier()),\n",
    "#         (\"SVM\", SVC(probability=True)),\n",
    "#         (\"NN\", MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)),\n",
    "#     ],\n",
    "#     voting=\"soft\",\n",
    "# )\n",
    "# evaluate_model(\n",
    "#     \"Voting (DT + SVM + NN)\", voting1, X_train, y_train, X_test, y_test, results\n",
    "# )\n",
    "\n",
    "# 7. Voting Classifier (RF, LR, KNN) \n",
    "# voting2 = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         (\"RF\", RandomForestClassifier(n_estimators=100)),\n",
    "#         (\"LR\", LogisticRegression(max_iter=1000)),\n",
    "#         (\"KNN\", KNeighborsClassifier()),\n",
    "#     ],\n",
    "#     voting=\"soft\",\n",
    "# )\n",
    "# evaluate_model(\n",
    "#     \"Voting (RF + LR + KNN)\", voting2, X_train, y_train, X_test, y_test, results\n",
    "# )\n",
    "\n",
    "# 8. Bagging \n",
    "# evaluate_model(\n",
    "#     \"Bagging (DT, 100)\",\n",
    "#     BaggingClassifier(\n",
    "#         estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42\n",
    "#     ),\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     results,\n",
    "# )\n",
    "\n",
    "# 9. MLP Classifier \n",
    "# evaluate_model(\n",
    "#     \"MLP Classifier\",\n",
    "#     MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     results,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6b49d",
   "metadata": {},
   "source": [
    "#### **Section 6. Compare Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c61986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Accuracy Gap</th>\n",
       "      <th>F1 Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88750</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.866056</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.133944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting (100)</td>\n",
       "      <td>0.960125</td>\n",
       "      <td>0.85625</td>\n",
       "      <td>0.95841</td>\n",
       "      <td>0.841106</td>\n",
       "      <td>0.103875</td>\n",
       "      <td>0.117304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Train Accuracy  Test Accuracy  Train F1   Test F1  \\\n",
       "0      Random Forest (100)        1.000000        0.88750   1.00000  0.866056   \n",
       "1  Gradient Boosting (100)        0.960125        0.85625   0.95841  0.841106   \n",
       "\n",
       "   Accuracy Gap    F1 Gap  \n",
       "0      0.112500  0.133944  \n",
       "1      0.103875  0.117304  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a table of results \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "######################################\n",
    "# Recommendation: See if you can add gap calculations \n",
    "# to your results and sort the table by test accuracy \n",
    "# to find the best models more efficiently. \n",
    "######################################\n",
    "\n",
    "results_df[\"Accuracy Gap\"] = results_df[\"Train Accuracy\"] - results_df[\"Test Accuracy\"]\n",
    "results_df[\"F1 Gap\"] = results_df[\"Train F1\"] - results_df[\"Test F1\"]\n",
    "\n",
    "results_df = results_df.sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\nSummary of All Models:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc691b",
   "metadata": {},
   "source": [
    "#### **Section 7. Conclusions and Insights**\n",
    "\n",
    "**Model 1: Random Forest (100)**\n",
    "\n",
    "**Train Accuracy:** 1.000000 (100%)\n",
    "The model perfectly predicts the training data, which is a sign of potential overfitting.\n",
    "\n",
    "**Test Accuracy:** 0.88750 (88.75%)\n",
    "The model performs well on unseen data but not as perfectly as on the training data.\n",
    "\n",
    "**Accuracy Gap:** 0.112500 (11.25%)\n",
    "The gap between training and test accuracy indicates some overfitting, as the model performs significantly better on the training data.\n",
    "\n",
    "**Train F1:** 1.00000 (100%)\n",
    "Perfect F1 score on the training data, further confirming overfitting.\n",
    "\n",
    "**Test F1:** 0.866056 (86.61%)\n",
    "The F1 score on the test data is slightly lower, reflecting the model's reduced performance on unseen data.\n",
    "\n",
    "**F1 Gap:** 0.133944 (13.39%)\n",
    "The gap between training and test F1 scores also indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277eae7d",
   "metadata": {},
   "source": [
    "#### **Model 2: Gradient Boosting (100)**\n",
    "\n",
    "**Train Accuracy:** 0.960125 (96.01%)\n",
    "The model performs very well on the training data but does not overfit as much as Random Forest.\n",
    "\n",
    "**Test Accuracy:** 0.85625 (85.63%)\n",
    "Slightly lower than Random Forest, but still a strong performance on unseen data.\n",
    "\n",
    "**Accuracy Gap:** 0.103875 (10.39%)\n",
    "The gap is smaller than Random Forest's, suggesting better generalization to unseen data.\n",
    "\n",
    "**Train F1:** 0.95841 (95.84%)\n",
    "High F1 score on the training data, indicating good balance between precision and recall.\n",
    "\n",
    "**Test F1:** 0.841106 (84.11%)\n",
    "Slightly lower than the training F1 score, but still strong on unseen data.\n",
    "\n",
    "**F1 Gap:** 0.117304 (11.73%)\n",
    "Smaller than Random Forest's F1 gap, again suggesting better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc1fd5",
   "metadata": {},
   "source": [
    "#### **Comparison of Analysis**\n",
    "\n",
    "**Random Forest (100)**\n",
    "\n",
    "- Performance:\n",
    "  \n",
    "Slightly better on the test set in terms of accuracy and F1 score.\n",
    "\n",
    "- Overfitting:\n",
    "Shows more overfitting, as indicated by the larger accuracy and F1 gaps.\n",
    "\n",
    "**Gradient Boosting (100)**\n",
    "\n",
    "- Performance:\n",
    "Slightly lower test performance compared to Random Forest.\n",
    "- Generalization:\n",
    "Generalizes better, as shown by smaller gaps between training and test metrics.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Random Forest (100) is a strong performer but shows signs of overfitting.\n",
    "Gradient Boosting (100) generalizes better and might be more reliable for unseen data, even if its test performance is slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d760b",
   "metadata": {},
   "source": [
    "Results from Sandra Ruiz\n",
    "\n",
    "(https://github.com/S572396/ml-05-sruiz/blob/main/ensemble-sruiz.ipynb)\n",
    "\n",
    "Model\tTrain Accuracy\tTest Accuracy\n",
    "Random Forest (100)\t1.000000\t0.8875\n",
    "AdaBoost (100)\t0.834246\t0.8250\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
